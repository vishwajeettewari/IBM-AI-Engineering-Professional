{"cells": [{"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n", "execution_count": 45, "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.Markdown object>", "text/markdown": "# <span style=\"color:red\"><<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>></span>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyspark==2.4.5\n", "execution_count": 46, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\r\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark==2.4.5) (0.10.7)\r\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 47, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()\n", "execution_count": 48, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(range(100))", "execution_count": 49, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# please replace $$ with the correct characters\nrdd.count()", "execution_count": 50, "outputs": [{"output_type": "execute_result", "execution_count": 50, "data": {"text/plain": "100"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.take(10)", "execution_count": 51, "outputs": [{"output_type": "execute_result", "execution_count": 51, "data": {"text/plain": "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.collect()", "execution_count": 52, "outputs": [{"output_type": "execute_result", "execution_count": 52, "data": {"text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.map(lambda x : x+1).take(10)", "execution_count": 53, "outputs": [{"output_type": "execute_result", "execution_count": 53, "data": {"text/plain": "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# sc.parallelize(range(1,101)).reduce(lambda a,b: a+b)", "execution_count": 54, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# 2- Functional Programming Basics with RDD"}, {"metadata": {}, "cell_type": "code", "source": "def gt50(i):\n    if i > 50:\n        return True\n    else:\n        return False", "execution_count": 11, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(4))\nprint(gt50(51))", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "False\nTrue\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "def gt50(i):\n    return i > 50\n", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(4))\nprint(gt50(51))", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "False\nTrue\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "gt50 = lambda i: i > 50", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(gt50(4))\nprint(gt50(51))", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "False\nTrue\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "#let's shuffle our list to make it a bit more interesting\nfrom random import shuffle\nl = list(range(100))\nshuffle(l)\nrdd = sc.parallelize(l)", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(gt50).collect()", "execution_count": 18, "outputs": [{"output_type": "execute_result", "execution_count": 18, "data": {"text/plain": "[96,\n 66,\n 61,\n 88,\n 60,\n 51,\n 52,\n 95,\n 82,\n 91,\n 98,\n 65,\n 83,\n 56,\n 59,\n 80,\n 63,\n 57,\n 68,\n 71,\n 53,\n 58,\n 55,\n 67,\n 77,\n 87,\n 99,\n 97,\n 94,\n 90,\n 79,\n 92,\n 70,\n 76,\n 72,\n 93,\n 69,\n 54,\n 75,\n 64,\n 86,\n 84,\n 73,\n 81,\n 62,\n 89,\n 78,\n 85,\n 74]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(gt50).take(10)", "execution_count": 19, "outputs": [{"output_type": "execute_result", "execution_count": 19, "data": {"text/plain": "[96, 66, 61, 88, 60, 51, 52, 95, 82, 91]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(lambda i: i > 50).collect()", "execution_count": 20, "outputs": [{"output_type": "execute_result", "execution_count": 20, "data": {"text/plain": "[96,\n 66,\n 61,\n 88,\n 60,\n 51,\n 52,\n 95,\n 82,\n 91,\n 98,\n 65,\n 83,\n 56,\n 59,\n 80,\n 63,\n 57,\n 68,\n 71,\n 53,\n 58,\n 55,\n 67,\n 77,\n 87,\n 99,\n 97,\n 94,\n 90,\n 79,\n 92,\n 70,\n 76,\n 72,\n 93,\n 69,\n 54,\n 75,\n 64,\n 86,\n 84,\n 73,\n 81,\n 62,\n 89,\n 78,\n 85,\n 74]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd.filter(lambda x: x>50).filter(lambda x: x<75).sum()", "execution_count": 21, "outputs": [{"output_type": "execute_result", "execution_count": 21, "data": {"text/plain": "1500"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# 3- Working with DataFrames"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import Row\n\ndf = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])\n\n# let's have a look what's inside\ndf.show()\n\n# let's print the schema\ndf.printSchema()", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "+---+------+\n| id| value|\n+---+------+\n|  1|value1|\n|  2|value2|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- value: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# register dataframe as query table\ndf.createOrReplaceTempView('df_view')\n\n# execute SQL query\ndf_result = spark.sql('select value from df_view where id=2')\n\n# examine contents of result\ndf_result.show()\n\n# get result as string\ndf_result.first().value", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "+------+\n| value|\n+------+\n|value2|\n+------+\n\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 23, "data": {"text/plain": "'value2'"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df.count()", "execution_count": 24, "outputs": [{"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "2"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "# Statistical Analysis"}, {"metadata": {}, "cell_type": "code", "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "--2020-08-22 06:41:03--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.118.4\nConnecting to github.com (github.com)|140.82.118.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2020-08-22 06:41:03--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2020-08-22 06:41:03--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\n100%[======================================>] 932,997     --.-K/s   in 0.05s   \n\n2020-08-22 06:41:04 (16.5 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.show()\ndf.printSchema()", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 20 rows\n\nroot\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql('select class,count(*) from df group by class').show()", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "+--------------+--------+\n|         class|count(1)|\n+--------------+--------+\n| Use_telephone|   15225|\n| Standup_chair|   25417|\n|      Eat_meat|   31236|\n|     Getup_bed|   45801|\n|   Drink_glass|   42792|\n|    Pour_water|   41673|\n|     Comb_hair|   23504|\n|          Walk|   92254|\n|  Climb_stairs|   40258|\n| Sitdown_chair|   25036|\n|   Liedown_bed|   11446|\n|Descend_stairs|   15375|\n|   Brush_teeth|   29829|\n|      Eat_soup|    6683|\n+--------------+--------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df.groupBy('class').count().show()", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n| Use_telephone|15225|\n| Standup_chair|25417|\n|      Eat_meat|31236|\n|     Getup_bed|45801|\n|   Drink_glass|42792|\n|    Pour_water|41673|\n|     Comb_hair|23504|\n|          Walk|92254|\n|  Climb_stairs|40258|\n| Sitdown_chair|25036|\n|   Liedown_bed|11446|\n|Descend_stairs|15375|\n|   Brush_teeth|29829|\n|      Eat_soup| 6683|\n+--------------+-----+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Pixiedust database opened successfully\nTable VERSION_TRACKER created successfully\nTable METRICS_TRACKER created successfully\n\nShare anonymous install statistics? (opt-out instructions)\n\nPixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n\n{\n   \"data_sent\": currentDate,\n   \"runtime\": \"python\",\n   \"application_version\": currentPixiedustVersion,\n   \"space_id\": nonIdentifyingUniqueId,\n   \"config\": {\n       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n       \"target_runtimes\": [\"Data Science Experience\"],\n       \"event_id\": \"web\",\n       \"event_organizer\": \"dev-journeys\"\n   }\n}\nYou can opt out by calling pixiedust.optOut() in a new cell.\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "\n        <div style=\"margin:10px\">\n            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n            </a>\n            <span>Pixiedust version 1.1.18</span>\n        </div>\n        "}, "metadata": {}}, {"output_type": "stream", "text": "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\nTable SPARK_PACKAGES created successfully\nTable USER_PREFERENCES created successfully\nTable service_connections created successfully\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "DataFrame[class: string, count: bigint]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql('''\n    select \n        *,\n        max/min as minmaxratio -- compute minmaxratio based on previously computed values\n        from (\n            select \n                min(ct) as min, -- compute minimum value of all classes\n                max(ct) as max, -- compute maximum value of all classes\n                mean(ct) as mean, -- compute mean between all classes\n                stddev(ct) as stddev -- compute standard deviation between all classes\n                from (\n                    select\n                        count(*) as ct -- count the number of rows per class and rename it to ct\n                        from df -- access the temporary query table called df backed by DataFrame df\n                        group by class -- aggrecate over class\n                )\n        )   \n''').show()", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import col, min, max, mean, stddev\n\ndf \\\n    .groupBy('class') \\\n    .count() \\\n    .select([ \n        min(col(\"count\")).alias('min'), \n        max(col(\"count\")).alias('max'), \n        mean(col(\"count\")).alias('mean'), \n        stddev(col(\"count\")).alias('stddev') \n    ]) \\\n    .select([\n        col('*'),\n        (col(\"max\") / col(\"min\")).alias('minmaxratio')\n    ]) \\\n    .show()", "execution_count": 31, "outputs": [{"output_type": "stream", "text": "+----+-----+------------------+------------------+-----------------+\n| min|  max|              mean|            stddev|      minmaxratio|\n+----+-----+------------------+------------------+-----------------+\n|6683|92254|31894.928571428572|21284.893716741157|13.80427951518779|\n+----+-----+------------------+------------------+-----------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql('select class,count(*) from df group by class').show()", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "+--------------+--------+\n|         class|count(1)|\n+--------------+--------+\n| Use_telephone|   15225|\n| Standup_chair|   25417|\n|      Eat_meat|   31236|\n|     Getup_bed|   45801|\n|   Drink_glass|   42792|\n|    Pour_water|   41673|\n|     Comb_hair|   23504|\n|          Walk|   92254|\n|  Climb_stairs|   40258|\n| Sitdown_chair|   25036|\n|   Liedown_bed|   11446|\n|Descend_stairs|   15375|\n|   Brush_teeth|   29829|\n|      Eat_soup|    6683|\n+--------------+--------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "df_count = df.groupBy('class').count().orderBy('count')", "execution_count": 33, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df_count.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 34, "outputs": [{"output_type": "display_data", "data": {"text/plain": "DataFrame[class: string, count: bigint]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 35, "outputs": [{"output_type": "display_data", "data": {"text/plain": "DataFrame[class: string, count: bigint]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "df.groupBy('class').count().orderBy('count').show()", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "+--------------+-----+\n|         class|count|\n+--------------+-----+\n|      Eat_soup| 6683|\n|   Liedown_bed|11446|\n| Use_telephone|15225|\n|Descend_stairs|15375|\n|     Comb_hair|23504|\n| Sitdown_chair|25036|\n| Standup_chair|25417|\n|   Brush_teeth|29829|\n|      Eat_meat|31236|\n|  Climb_stairs|40258|\n|    Pour_water|41673|\n|   Drink_glass|42792|\n|     Getup_bed|45801|\n|          Walk|92254|\n+--------------+-----+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import pixiedust\nfrom pyspark.sql.functions import col\ncounts = df.groupBy('class').count().orderBy('count')\ndisplay(counts)", "execution_count": 37, "outputs": [{"output_type": "display_data", "data": {"text/plain": "DataFrame[class: string, count: bigint]"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.functions import min\n\n# create a lot of distinct classes from the dataset\nclasses = [row[0] for row in df.select('class').distinct().collect()]\n\n# compute the number of elements of the smallest class in order to limit the number of samples per calss\nmin = df.groupBy('class').count().select(min('count')).first()[0]\n\n# define the result dataframe variable\ndf_balanced = None\n\n# iterate over distinct classes\nfor cls in classes:\n    \n    # only select examples for the specific class within this iteration\n    # shuffle the order of the elements (by setting fraction to 1.0 sample works like shuffle)\n    # return only the first n samples\n    df_temp = df \\\n        .filter(\"class = '\"+cls+\"'\") \\\n        .sample(False, 1.0) \\\n        .limit(min)\n    \n    # on first iteration, passing df_temp to empty df_balanced\n    if df_balanced == None:    \n        df_balanced = df_temp\n    # afterwards, append vertically\n    else:\n        df_balanced=df_balanced.union(df_temp)", "execution_count": 38, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nasa = [1,2,4,5,34,1,32,4,34,2,1,3]\nnp.median(asa)", "execution_count": 39, "outputs": [{"output_type": "execute_result", "execution_count": 39, "data": {"text/plain": "3.5"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "stdd = [34,1,23,4,3,3,12,4,3,1]\nnp.std(stdd)\n", "execution_count": 40, "outputs": [{"output_type": "execute_result", "execution_count": 40, "data": {"text/plain": "10.562196741208714"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "rdd = sc.parallelize(range(100))", "execution_count": 41, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from scipy.stats import kurtosis\nkurtosis([34,1,23,4,3,3,12,4,3,1])", "execution_count": 42, "outputs": [{"output_type": "execute_result", "execution_count": 42, "data": {"text/plain": "0.663124005193275"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics\n\ncolumn1 = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\ncolumn2 = sc.parallelize([7,6,5,4,5,6,7,8,9,10])\ndata2 = column1.zip(column2)\nStatistics.corr(data2)\n", "execution_count": 43, "outputs": [{"output_type": "execute_result", "execution_count": 43, "data": {"text/plain": "array([[1.        , 0.70927291],\n       [0.70927291, 1.        ]])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "data.take(10)", "execution_count": 44, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'data' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-44-f897984048d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "meanX = column1.sum()/column1.count()\nmeanY = column2.sum()/column2.count()\nprint (meanX)\nprint (meanY)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "x = [1,2,3,4,5,6,7,8,9,10]\ny = [7,6,5,4,5,6,7,8,9,10]\nnp.cov(x)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "rddX = sc.parallelize(range(11))\nrddY = sc.parallelize(range(4,11))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nmean_x = sum(x) / len(x)\nmean_y = sum(y) / len(y)\n\nsum((a - mean_x) * (b - mean_y) for (a,b) in zip(x,y)) / len(x)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}